import pandas as pd
import numpy as np
from collections import Counter
import re
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import random

# fixing random seed for reproducibility
random.seed(123)
np.random.seed(123)

#————————————————Load Raw texts and labels into arrays————————————————#
# 1. load data
data1_train=pd.read_csv('./data_sentiment/train.csv',header=None,names=["text","label"])
data1_dev=pd.read_csv('./data_sentiment/dev.csv',header=None,names=["text","label"])
data1_test=pd.read_csv('./data_sentiment/test.csv',header=None,names=["text","label"])
data1_train.head()

# 2. Make the raw texts into lists and their corresponding labels into np.arrays
def creat_list_array(data_text,data_label):
    x=data_text.tolist()
    y=np.array(data_label)
    return x,y
# Transform train data
data1_train_x,data1_train_y=creat_list_array(data1_train['text'],data1_train['label']) #len 1400
# Transform validation data
data1_dev_x,data1_dev_y=creat_list_array(data1_dev['text'],data1_dev['label']) #len 200
# Transform test data
data1_test_x,data1_test_y=creat_list_array(data1_test['text'],data1_test['label']) #len 400

#——————————————————————Bag-of-Words Representation——————————————————————#
# 1. N-gram extraction from a document

# set stop word
stop_words = ['a','in','on','at','and','or', 
              'to', 'the', 'of', 'an', 'by', 
              'as', 'is', 'was', 'were', 'been', 'be', 
              'are','for', 'this', 'that', 'these', 'those', 'you', 'i',
             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',
              'do', 'did', 'can', 'could', 'who', 'which', 'what', 
             'his', 'her', 'they', 'them', 'from', 'with', 'its']

# tokenise, create unigrams, using stop-words
def tokenise(data,token_pattern,stop_words):
    token_data=[]
    token_list=re.findall(token_pattern,data)
    for word in token_list:
        if word not in stop_words:
            token_data.append(word)
    return token_data

# based on the tokenised data(unigrams), create bigrams or trigrams
def ngrams_generate(data,n):
    result_list=[]
    ngrams = zip(*[data[i:] for i in range(n)])
    for ngram in ngrams:
        result_list.append((ngram))
    return result_list

# extract ngrams function
def extract_ngrams(x_raw,ngram_range,token_pattern=r'\b[A-Za-z][A-Za-z]+\b',stop_words=stop_words,vocab=set()):
    # tokenise data
    token_data=tokenise(x_raw,token_pattern=token_pattern,stop_words=stop_words)
    # create ngrams list which save ngrams result
    result_ngrams=[]
    result_vocab=[]
    # Extract ngrams based on the ngram_range
    if ngram_range[0]==1:
        result_ngrams=token_data
        for i in range(ngram_range[0],ngram_range[1]):
            ngrams=ngrams_generate(token_data,i+1)
            result_ngrams=result_ngrams+ngrams
    else:
        result_ngrams=ngrams_generate(token_data,ngram_range[0])
        for i in range(ngram_range[0],ngram_range[1]):
            ngrams=ngrams_generate(token_data,ngram_range[0]+1)
            result_ngrams=result_ngrams+ngrams
    # Extract specific vocab based on the vocab set()
    if len(vocab)==0:
        return result_ngrams
    
    else:
        for word in vocab:
            if word in result_ngrams:
                result_vocab.append(word)
        return result_vocab
    
# Extract ngrams on the complete data set, for dev and test sets
def extract_ngrams_for_test(X_data,ngram_range,stop_words=stop_words):
    # Extract ngrams from raw data
    ngrams_list_without_Ded=[]
    for i in range(len(X_data)):
        ngrams_data=extract_ngrams(X_data[i],ngram_range=ngram_range,stop_words=stop_words)
        ngrams_list_without_Ded.append(ngrams_data)
    return ngrams_list_without_Ded

# example
inputs='this is a great movie to watch'
extract_ngrams(inputs,ngram_range=(1,3),stop_words=stop_words,vocab=set())

extract_ngrams(inputs,ngram_range=(1,2),stop_words=stop_words, 
               vocab=set([('great','movie'),'great']))
               
# 2. Create a vocabulary of n-grams

def get_vocab(X_raw, ngram_range, min_df, keep_topN, stop_words):
    # Extract ngrams from raw data
    ngrams_list=[]
    ngrams_list_without_Ded=[]
    for i in range(len(X_raw)):
        ngrams_data=extract_ngrams(X_raw[i],ngram_range=ngram_range,stop_words=stop_words)
        ngrams_list_without_Ded.append(ngrams_data)
        # Deduplication
        ngrams_data_Ded=sorted(set(ngrams_data),key=ngrams_data.index)
        ngrams_list.append(ngrams_data_Ded)
    # create vocab dictionary
    vocab_dict={}
    for i in range(len(ngrams_list)):
        for word in ngrams_list[i]:
            if word in vocab_dict:
                vocab_dict[word]+=1
            else:
                # keep ngrams with a minimun df
                if min_df==0: 
                    vocab_dict[word]=1
                else:
                    vocab_dict[word]=min_df
    # sorted then keep only topN
    vocab_sorted=sorted(vocab_dict.items(),key=lambda item:item[1],reverse=True)
    vocab_topN=vocab_sorted[:keep_topN]
    vocab=[]
    for i in range(len(vocab_topN)):
        vocab.append(vocab_topN[i][0])
    return vocab,vocab_topN,ngrams_list_without_Ded
    
# 3. Extract ngrams of train, validation, test data separately

# extract vocab, df——train, train token data
vocab,df_tr,ngrams_without_Ded_tr=get_vocab(data1_train_x,ngram_range=(1,3),  
                                            min_df=0,keep_topN=5000, stop_words=stop_words)
# extract dev token
ngrams_without_Ded_dev=extract_ngrams_for_test(data1_dev_x,ngram_range=(1,3),stop_words=stop_words)
# extract test token
ngrams_without_Ded_test=extract_ngrams_for_test(data1_test_x,ngram_range=(1,3),stop_words=stop_words)

# example
print(len(vocab))
print()
print(random.sample(vocab,100))
print()
print(df_tr[:10])

# 4. Create tf for each text based on vocab for creating vector

def create_tf_dict(token_data,vocab):
    tf_dict={}
    # Record the term frequency of each text
    for i in range(len(token_data)):
        tf_dict[i]={}
        for word in token_data[i]:
            if word in vocab:
                if word in tf_dict[i]:
                    tf_dict[i][word]+=1
                else:
                    tf_dict[i][word]=1
    return tf_dict

# Create a dictionary of tf for train, dev, test data based on vocab
tf_dict_tr=create_tf_dict(ngrams_without_Ded_tr,vocab) #len 1400
tf_dict_dev=create_tf_dict(ngrams_without_Ded_dev,vocab) #len 200
tf_dict_test=create_tf_dict(ngrams_without_Ded_test,vocab) #len 400

#——————————————————————Vectorise documents——————————————————————#
# 1. Extract the idf of each word based on the training set data

def creat_idf(data,doc_num):
    # create dictionary of df {word->df}
    df_dict={}
    for i in range(len(data)):
        df_dict[data[i][0]]=data[i][1]
    # create dictionary of idf
    idf_dict={}
    for word in df_dict:
        idf_dict[word]=np.log(len(doc_num)/df_dict[word])
    return idf_dict

#"df_tr" is list-(word,df), "tf_dict_tr" is used to calculate the total number of doc
idf_dict_tr=creat_idf(df_tr,tf_dict_tr)

# idf example
print(list(idf_dict_tr.items())[:5])

# 2. Count vectors and tf.idf vectors

# Transform data into tf vector and tfidf vector according to vocab and idf dictionary
def vectorise(data,vocab,idf_dict_tr):
    # vectorise
    a=[]
    a_idf=[]
    for i in range(len(data)):
        b=[]
        b_idf=[]
        for word in vocab:
            if word in data[i]: 
                c=data[i][word] # data[i][word]->value is the tf of word
                c_idf=data[i][word]*idf_dict_tr[word] # tf*idf
            else:
                c=0
                c_idf=0
            b.append(c)
            b_idf.append(c_idf)
        a.append(b)
        a_idf.append(b_idf)
    array_tf=np.array(a)
    array_idf=np.array(a_idf)
    return array_tf,array_idf
    
tf_vect_tr,idf_vect_tr=vectorise(tf_dict_tr,vocab,idf_dict_tr) #train (1400,5000)
tf_vect_dev,idf_vect_dev=vectorise(tf_dict_dev,vocab,idf_dict_tr) #dev (200,5000)
tf_vect_test,idf_vect_test=vectorise(tf_dict_test,vocab,idf_dict_tr) #test (400,5000)

# tf vector example
tf_vect_dev[:2,:50]

# tfidf vector example
idf_vect_test[:2,:50]

#——————————————————————Binary Logistic Regression——————————————————————#
# 1. Create sigmoi function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
# sigmoid example
print(sigmoid(0)) 
print(sigmoid(np.array([-5., 1.2])))

# 2. Create functions that get predicted probability and predicted class
def predict_prob_class(X,weights):
    # Calculating prediction probability
    z=X@weights.T
    pred_prob_1=sigmoid(z) #shape (1400,1)
    pred_prob=np.squeeze(pred_prob_1,axis = None)
    # Get predicted class based on predicted probability
    pred_result_list=[]
    for i in range(pred_prob.size):
        if pred_prob[i]<0.5:
            pred_class = 0
        else:
            pred_class = 1
        pred_result_list.append(pred_class)
    pred_result=np.array(pred_result_list)
    return pred_prob,pred_result
    
# 3. Create a function that calculates binary loss
def binary_loss(X, Y, weights, alpha):
    # Get predicted probability
    pred_y_prob,pred_y_class = predict_prob_class(X,weights=weights)
    # Calculating the amount of data
    m=Y.shape[0]
    # Calculate the number of weight matrices
    m_w=weights.shape[1]
    # Calculate loss value
    loss = Y * np.log(pred_y_prob) + (1-Y)*np.log(1-pred_y_prob+10**(-10))
    cost1 = -np.sum(loss)/m
    # L2 Regularization
    l2_regularization_cost = alpha*(np.sum(np.square(weights)))/m_w/2
    return cost1+l2_regularization_cost
    
# 4. Create stochastic gradient descent (SGD) function
# Randomly shuffle the data set, used to randomise the order of training data in SGD process
def randmoise_data(data,label):
    index=[i for i in range(len(data))]
    random.shuffle(index)
    data1=data[index]
    label1=label[index]
    return data1,label1
   
def SGD(X_tr,Y_tr,X_dev,Y_dev,lr,alpha,epochs,tolerance,print_progress=True):
    cur_loss_tr = 1.
    cur_loss_dev = 1.
    training_loss_history = []
    validation_loss_history = []
    
    # Set initial weights
    weights=np.zeros([1,X_tr.shape[1]]) #(1,5000)

    #Start iteration
    for i in range(epochs):
        # calculate current dev loss
        cur_loss_dev=binary_loss(X_dev,Y_dev,weights,alpha=alpha)
        if print_progress==True:
            validation_loss_history.append(cur_loss_dev)
        
        # shuffle the data set
        X_tr_ran,Y_tr_ran=randmoise_data(X_tr, Y_tr)
        # Get predicted probability
        tr_pred_prob,tr_pred_class=predict_prob_class(X_tr_ran,weights)
        
        # update weights
        for n in range(len(X_tr)):
            # w_new = w - lr * ((P(y|xi,w) - yi) * xi) - (lr * alpha / len(w)) * w
            weights=weights-lr*((tr_pred_prob[n]-Y_tr_ran[n])*X_tr_ran[n])-(lr*alpha/weights.shape[1])*weights
            
        # calculate current train loss
        cur_loss_tr=binary_loss(X_tr_ran,Y_tr_ran,weights,alpha=alpha)
        if print_progress==True:
            training_loss_history.append(cur_loss_tr)
        
        # Print the data of the training process
        print('Epoch:{0}|Training Loss{1}|Validation Loss{2}'.format(i,cur_loss_tr,cur_loss_dev))
        # if diff smaller than tolerance then break iteration
        if i==0:
            pass
        else:
            # diff = previous validation loss − current validation loss
            diff=validation_loss_history[-2]-cur_loss_dev
            if abs(diff)<tolerance:
                break
    return weights, training_loss_history, validation_loss_history
    
# 5. Create (1) plot, (2) compute score, (3) print top-10 functions
# plot function
def plt_loss(y1,y2,epochs):
    x=np.linspace(1,epochs,epochs)
    plt.title('Training Monitoring')
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    #plt.ylim(0.2, 0.7)
    plt.plot(x, y1, color="blue", linewidth=1.0, linestyle="-",label='training loss')
    plt.plot(x, y2, color="red", linewidth=1.0, linestyle="-",label='validation loss')
    plt.legend(loc='upper left', bbox_to_anchor=(0.6, 0.95))
    plt.show()
# compute score function
def compute_score(test_data,y_true,weights):
    # Computing predicted class result
    test_pred_prob,test_pred_result=predict_prob_class(test_data,weights)
    print('Accuracy:', accuracy_score(y_true,test_pred_result))
    print('Precision:', precision_score(y_true,test_pred_result))
    print('Recall:', recall_score(y_true,test_pred_result))
    print('F1-Score:', f1_score(y_true,test_pred_result))
    
# print top-10 function
def Find_top10(vocab,w):
    top10_list=[]
    for word_index in range(len(vocab)):
        tuple_1=(vocab[word_index],w[0][word_index])
        top10_list.append(tuple_1)
    # rank list
    rank_list=sorted(top10_list,key = lambda value: value[1],reverse=True)
    # Take the first ten and the last ten
    top10_Pos_list=rank_list[:10]
    top10_Neg_list=rank_list[-10:]
    return top10_Pos_list,top10_Neg_list
def print_top10_word(Pos_list,Neg_list):
    print('Top-10 Positive words:')
    print()
    for word in Pos_list:
        print(word[0])
    print()
    print('Top-10 Negative words:')
    print()
    for word in Neg_list:
        print(word[0])


#——————————————————————Train and Evaluate Logistic Regression with Count vectors——————————————————————#
# 1.1 (tf_vector) Model 1——lr = 0.001, alpha = 0.001, epochs = 100
w_count_T1_M1, loss_tr_count_T1_M1, loss_dev_count_T1_M1=SGD(tf_vect_tr, data1_train_y,   
                                                                      tf_vect_dev, data1_dev_y,
                                                                       lr=0.001, 
                                                                       alpha=0.001, 
                                                                       epochs=100, 
                                                                       tolerance=0.00001,
                                                                       print_progress=True)
# plot the training and validation history per epoch
plt_loss(loss_tr_count_T1_M1,loss_dev_count_T1_M1,epochs=100)

# Compute accuracy, precision, recall and F1-scores
compute_score(tf_vect_test,data1_test_y,w_count_T1_M1)
'''
result:
Accuracy: 0.825
Precision: 0.8217821782178217
Recall: 0.83
F1-Score: 0.8258706467661692
'''
# Print the top-10 words for the negative and positive class respectively
Top10_Pos_T1_tf_M1,Top10_Neg_T1_tf_M1=Find_top10(vocab,w_count_T1_M1)
print_top10_word(Top10_Pos_T1_tf_M1,Top10_Neg_T1_tf_M1)

# 1.2 (tf_vector) Model 2——lr = 0.001, alpha = 0.001, epochs = 50
w_count_T1_M2, loss_tr_count_T1_M2, loss_dev_count_T1_M2=SGD(tf_vect_tr, data1_train_y,   
                                                                      tf_vect_dev, data1_dev_y,
                                                                       lr=0.001, 
                                                                       alpha=0.001, 
                                                                       epochs=50, 
                                                                       tolerance=0.00001,
                                                                       print_progress=True)
plt_loss(loss_tr_count_T1_M2,loss_dev_count_T1_M2,epochs=50)
compute_score(tf_vect_test,data1_test_y,w_count_T1_M2)
'''
result:
Accuracy: 0.8075
Precision: 0.8360655737704918
Recall: 0.765
F1-Score: 0.7989556135770236
'''
Top10_Pos_T1_tf_M2,Top10_Neg_T1_tf_M2=Find_top10(vocab,w_count_T1_M2)
print_top10_word(Top10_Pos_T1_tf_M2,Top10_Neg_T1_tf_M2)

#——————————————————————Train and Evaluate Logistic Regression with TF.IDF vectors——————————————————————#
# 2.1 (tf.idf_vector) Model 1——lr = 0.0001, alpha = 0.00001, epochs = 50
w_tfidf_T1_M1,loss_tr_count_tfidf_T1_M1,loss_dev_count_tfidf_T1_M1 = SGD(idf_vect_tr, data1_train_y, 
                                                                         idf_vect_dev, data1_dev_y, 
                                                                         lr=0.0001, 
                                                                         alpha=0.00001, 
                                                                         epochs=50,  
                                                                         tolerance=0.00001,
                                                                         print_progress=True)
plt_loss(loss_tr_count_tfidf_T1_M1,loss_dev_count_tfidf_T1_M1,epochs=50)
compute_score(idf_vect_test,data1_test_y,w_tfidf_T1_M1)
'''
result:
Accuracy: 0.8725
Precision: 0.8634146341463415
Recall: 0.885
F1-Score: 0.874074074074074
'''
Top10_Pos_idf_T1_M1,Top10_Neg_idf_T1_M1=Find_top10(vocab,w_tfidf_T1_M1)
print_top10_word(Top10_Pos_idf_T1_M1,Top10_Neg_idf_T1_M1)

# 2.2 (tf.idf_vector) Model 2——lr = 0.00001, alpha = 0.00001, epochs = 50
w_tfidf_T1_M2,loss_tr_count_tfidf_T1_M2,loss_dev_count_tfidf_T1_M2 = SGD(idf_vect_tr, data1_train_y, 
                                                                         idf_vect_dev, data1_dev_y, 
                                                                         lr=0.00001, 
                                                                         alpha=0.00001, 
                                                                         epochs=50,  
                                                                         tolerance=0.00001,
                                                                         print_progress=True)
plt_loss(loss_tr_count_tfidf_T1_M2,loss_dev_count_tfidf_T1_M2,epochs=50)
compute_score(idf_vect_test,data1_test_y,w_tfidf_T1_M2)
'''
result:
Accuracy: 0.8625
Precision: 0.8502415458937198
Recall: 0.88
F1-Score: 0.8648648648648648
'''
Top10_Pos_idf_T1_M2,Top10_Neg_idf_T1_M2=Find_top10(vocab,w_tfidf_T1_M2)
print_top10_word(Top10_Pos_idf_T1_M2,Top10_Neg_idf_T1_M2)

# 2.3 (tf.idf_vector) Model 3——lr = 0.0001, alpha = 0.0001, epochs = 50
w_tfidf_T1_M3,loss_tr_count_tfidf_T1_M3,loss_dev_count_tfidf_T1_M3 = SGD(idf_vect_tr, data1_train_y, 
                                                                         idf_vect_dev, data1_dev_y, 
                                                                         lr=0.0001, 
                                                                         alpha=0.0001, 
                                                                         epochs=50,  
                                                                         tolerance=0.00001,
                                                                         print_progress=True)
plt_loss(loss_tr_count_tfidf_T1_M3,loss_dev_count_tfidf_T1_M3,epochs=50)
compute_score(idf_vect_test,data1_test_y,w_tfidf_T1_M3)
'''
result:
Accuracy: 0.8725
Precision: 0.8634146341463415
Recall: 0.885
F1-Score: 0.874074074074074
'''
Top10_Pos_idf_T1_M3,Top10_Neg_idf_T1_M3=Find_top10(vocab,w_tfidf_T1_M3)
print_top10_word(Top10_Pos_idf_T1_M3,Top10_Neg_idf_T1_M3)



